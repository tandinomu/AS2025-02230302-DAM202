{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b572eef",
   "metadata": {},
   "source": [
    "## **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e9a76",
   "metadata": {},
   "source": [
    "**Text preprocessing** involves several sequential steps that clean and normalize textual data. Raw text often contains inconsistencies, noise, and irrelevant information that can negatively impact model performance. Through systematic preprocessing, we can extract meaningful features and improve the quality of our text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9f84b9-87b7-48cd-ad1b-dd6d7ccfc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./lib/python3.9/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in ./lib/python3.9/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./lib/python3.9/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./lib/python3.9/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./lib/python3.9/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./lib/python3.9/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./lib/python3.9/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./lib/python3.9/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./lib/python3.9/site-packages (from spacy) (0.17.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./lib/python3.9/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./lib/python3.9/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./lib/python3.9/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./lib/python3.9/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.9/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.9/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./lib/python3.9/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in ./lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.9/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas \n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c140898",
   "metadata": {},
   "source": [
    "## **Section 0: Creating Data Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b5fc2a-1bad-4cf4-a4e4-e9ca80b3886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Pandas library\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcecfe",
   "metadata": {},
   "source": [
    "**Purpose:** Import the essential pandas library for data manipulation and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1a2b51-55c6-4e37-b0fd-d5afa195649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "\n",
    "\"When life gives you lemons, make lemonade! üôÇ\",\n",
    "\n",
    "\"She bought 2 lemons for $1 at Maven Market.\",\n",
    "\n",
    "\"A dozen lemons will make a gallon of lemonade. [AllRecipes]\",\n",
    "\n",
    "\"lemon, lemon, lemons, lemon, lemon, lemons\",\n",
    "\n",
    "\"He's running to the market to get a lemon ‚Äî there's a great sale today.\",\n",
    "\n",
    "\"Does Maven Market carry Eureka lemons or Meyer lemons?\",\n",
    "\n",
    "\"An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]\",\n",
    "\n",
    "\"iced tea is my favorite\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71172126",
   "metadata": {},
   "source": [
    "**Purpose:** Create a sample dataset with diverse text challenges including:\n",
    "\n",
    "- Mixed case letters\n",
    "- Punctuation marks\n",
    "- Special characters and emojis\n",
    "- Numbers and currency symbols\n",
    "- Contractions and apostrophes\n",
    "- Citations in brackets\n",
    "- Repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa4d9b9-7934-4261-913f-3712679c22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert list to DataFrame\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed596c6",
   "metadata": {},
   "source": [
    "**Purpose:** Transform the list into a pandas DataFrame for easier manipulation and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a434be5-7a60-407b-87f5-991f6fd144ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set display options to show full content\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05581221",
   "metadata": {},
   "source": [
    "**Purpose:** Configure pandas to display complete text content without truncation, essential for examining preprocessing results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897ba0d",
   "metadata": {},
   "source": [
    "## **Section 1: Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b5570-a952-4d79-bfe1-d83d228f3398",
   "metadata": {},
   "source": [
    "### **Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb28e4",
   "metadata": {},
   "source": [
    "**Text normalization** is the process of converting text to a standard, consistent format. The most common normalization technique is converting all text to lowercase, which ensures that words like \"Apple\" and \"apple\" are treated as the same token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1edc0e-4a2e-4773-a824-405dcf1761d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a copy for spaCy processing\n",
    "\n",
    "spacy_df = data_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Convert text to lowercase\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['sentence'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f663e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! üôÇ</td>\n",
       "      <td>when life gives you lemons, make lemonade! üôÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "      <td>she bought 2 lemons for $1 at maven market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade. [AllRecipes]</td>\n",
       "      <td>a dozen lemons will make a gallon of lemonade. [allrecipes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon ‚Äî there's a great sale today.</td>\n",
       "      <td>he's running to the market to get a lemon ‚Äî there's a great sale today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does Maven Market carry Eureka lemons or Meyer lemons?</td>\n",
       "      <td>does maven market carry eureka lemons or meyer lemons?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]</td>\n",
       "      <td>an arnold palmer is half lemonade, half iced tea. [wikipedia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "      <td>iced tea is my favorite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  sentence  \\\n",
       "0                             When life gives you lemons, make lemonade! üôÇ   \n",
       "1                              She bought 2 lemons for $1 at Maven Market.   \n",
       "2              A dozen lemons will make a gallon of lemonade. [AllRecipes]   \n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons   \n",
       "4  He's running to the market to get a lemon ‚Äî there's a great sale today.   \n",
       "5                   Does Maven Market carry Eureka lemons or Meyer lemons?   \n",
       "6            An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]   \n",
       "7                                                  iced tea is my favorite   \n",
       "\n",
       "                                                            clean_sentence  \n",
       "0                             when life gives you lemons, make lemonade! üôÇ  \n",
       "1                              she bought 2 lemons for $1 at maven market.  \n",
       "2              a dozen lemons will make a gallon of lemonade. [allrecipes]  \n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons  \n",
       "4  he's running to the market to get a lemon ‚Äî there's a great sale today.  \n",
       "5                   does maven market carry eureka lemons or meyer lemons?  \n",
       "6            an arnold palmer is half lemonade, half iced tea. [wikipedia]  \n",
       "7                                                  iced tea is my favorite  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d797205",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "- Create a working copy to preserve original data\n",
    "- Convert all text to lowercase for consistency\n",
    "- Store results in a new column called 'clean_sentence'\n",
    "\n",
    "**Key Learning:** Normalization reduces vocabulary size and prevents case-sensitive duplicates from being treated as different words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530256",
   "metadata": {},
   "source": [
    "### **Text Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9206495-b923-4bbd-ae28-d59676e40abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove specific citations\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace('[wikipedia]', '')\n",
    "\n",
    "\n",
    "\n",
    "# Advanced cleaning with regex\n",
    "\n",
    "combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(combined, ' ', regex=True)\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76b21c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! üôÇ</td>\n",
       "      <td>when life gives you lemons make lemonade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "      <td>she bought 2 lemons for 1 at maven market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade. [AllRecipes]</td>\n",
       "      <td>a dozen lemons will make a gallon of lemonade allrecipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "      <td>lemon lemon lemons lemon lemon lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon ‚Äî there's a great sale today.</td>\n",
       "      <td>he s running to the market to get a lemon there s a great sale today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does Maven Market carry Eureka lemons or Meyer lemons?</td>\n",
       "      <td>does maven market carry eureka lemons or meyer lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]</td>\n",
       "      <td>an arnold palmer is half lemonade half iced tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "      <td>iced tea is my favorite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  sentence  \\\n",
       "0                             When life gives you lemons, make lemonade! üôÇ   \n",
       "1                              She bought 2 lemons for $1 at Maven Market.   \n",
       "2              A dozen lemons will make a gallon of lemonade. [AllRecipes]   \n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons   \n",
       "4  He's running to the market to get a lemon ‚Äî there's a great sale today.   \n",
       "5                   Does Maven Market carry Eureka lemons or Meyer lemons?   \n",
       "6            An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]   \n",
       "7                                                  iced tea is my favorite   \n",
       "\n",
       "                                                         clean_sentence  \n",
       "0                              when life gives you lemons make lemonade  \n",
       "1                             she bought 2 lemons for 1 at maven market  \n",
       "2              a dozen lemons will make a gallon of lemonade allrecipes  \n",
       "3                                 lemon lemon lemons lemon lemon lemons  \n",
       "4  he s running to the market to get a lemon there s a great sale today  \n",
       "5                 does maven market carry eureka lemons or meyer lemons  \n",
       "6                       an arnold palmer is half lemonade half iced tea  \n",
       "7                                               iced tea is my favorite  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa43aeb",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Remove citations and references\n",
    "- Use regular expressions to remove URLs, email addresses, social media handles, and non-alphanumeric characters\n",
    "- Normalize whitespace by replacing multiple spaces with single spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c643b",
   "metadata": {},
   "source": [
    "## **Section 1.2: Advanced Text Processing with spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0520ebc",
   "metadata": {},
   "source": [
    "**spaCy** is an advanced NLP library that intelligently processes text by understanding grammar and word relationships, providing better tokenization and lemmatization than basic string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f78a01f-2847-4419-8709-9e6900162d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookairm4chip/Desktop/DAM202-SEM5/Practical1/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbookairm4chip/Desktop/DAM202-SEM5/Practical1/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m75.3 kB/s\u001b[0m  \u001b[33m0:02:30\u001b[0mm0:00:03\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Download and install English language model\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained pipeline\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "# Process a sample sentence\n",
    "\n",
    "phrase = spacy_df.clean_sentence[0] # \"when life gives you lemons make lemonade\"\n",
    "\n",
    "doc = nlp(phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e72bad",
   "metadata": {},
   "source": [
    "### **1.2.1 Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8e964",
   "metadata": {},
   "source": [
    "**Tokenization** splits text into individual units (tokens) such as words, punctuation marks, or numbers. Modern tokenizers handle complex cases like contractions, compound words, and special characters intelligently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8caf1469-59ad-4f2b-ae93-7cd0629a5210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[when, life, gives, you, lemons, make, lemonade]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract tokens as text strings\n",
    "\n",
    "[token.text for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'gives', 'you', 'lemons', 'make', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Extract tokens as spaCy objects (with linguistic attributes)\n",
    "\n",
    "[token for token in doc]\n",
    "\n",
    "# Output: [when, life, gives, you, lemons, make, lemonade]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2644a51",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Demonstrate two ways to access tokens\n",
    "- Show how spaCy preserves linguistic information in token objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0c3cd",
   "metadata": {},
   "source": [
    "### **1.2.2 Lemmatization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38fbda",
   "metadata": {},
   "source": [
    "**Lemmatization** reduces words to their base or root form (lemma) using linguistic knowledge. Unlike stemming, which simply removes suffixes, lemmatization considers the word's part of speech and meaning to find the correct root form.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- \"running\" ‚Üí \"run\"\n",
    "- \"better\" ‚Üí \"good\"\n",
    "- \"mice\" ‚Üí \"mouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f1d5fd-3074-4684-a299-b1ba5fe4c6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract lemmatized forms\n",
    "\n",
    "[token.lemma_ for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020e53f",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Convert words to their dictionary forms\n",
    "- Reduce vocabulary size by grouping inflected forms\n",
    "- Note how \"gives\" becomes \"give\" and \"lemons\" becomes \"lemon\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b697f40",
   "metadata": {},
   "source": [
    "### **1.2.3 Stop Words Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fa38a",
   "metadata": {},
   "source": [
    "**Stop words** are common words that carry little semantic meaning and are often filtered out to focus on more meaningful content. Examples include \"the\", \"and\", \"is\", \"in\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64a2c061-5bf9-460a-a7ae-62c1b6767381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life give lemon lemonade'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# View all English stop words in spaCy\n",
    "\n",
    "list(nlp.Defaults.stop_words)\n",
    "\n",
    "print(f\"Total stop words: {len(list(nlp.Defaults.stop_words))}\") # 326 stop words\n",
    "\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "[token for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: [life, gives, lemons, lemonade]\n",
    "\n",
    "\n",
    "\n",
    "# Combine lemmatization and stop word removal\n",
    "\n",
    "[token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: ['life', 'give', 'lemon', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Convert back to sentence format\n",
    "\n",
    "norm = [token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "' '.join(norm) # Output: 'life give lemon lemonade'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b257bdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! üôÇ</td>\n",
       "      <td>when life gives you lemons make lemonade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "      <td>she bought 2 lemons for 1 at maven market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade. [AllRecipes]</td>\n",
       "      <td>a dozen lemons will make a gallon of lemonade allrecipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "      <td>lemon lemon lemons lemon lemon lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon ‚Äî there's a great sale today.</td>\n",
       "      <td>he s running to the market to get a lemon there s a great sale today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does Maven Market carry Eureka lemons or Meyer lemons?</td>\n",
       "      <td>does maven market carry eureka lemons or meyer lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]</td>\n",
       "      <td>an arnold palmer is half lemonade half iced tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "      <td>iced tea is my favorite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  sentence  \\\n",
       "0                             When life gives you lemons, make lemonade! üôÇ   \n",
       "1                              She bought 2 lemons for $1 at Maven Market.   \n",
       "2              A dozen lemons will make a gallon of lemonade. [AllRecipes]   \n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons   \n",
       "4  He's running to the market to get a lemon ‚Äî there's a great sale today.   \n",
       "5                   Does Maven Market carry Eureka lemons or Meyer lemons?   \n",
       "6            An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]   \n",
       "7                                                  iced tea is my favorite   \n",
       "\n",
       "                                                         clean_sentence  \n",
       "0                              when life gives you lemons make lemonade  \n",
       "1                             she bought 2 lemons for 1 at maven market  \n",
       "2              a dozen lemons will make a gallon of lemonade allrecipes  \n",
       "3                                 lemon lemon lemons lemon lemon lemons  \n",
       "4  he s running to the market to get a lemon there s a great sale today  \n",
       "5                 does maven market carry eureka lemons or meyer lemons  \n",
       "6                       an arnold palmer is half lemonade half iced tea  \n",
       "7                                               iced tea is my favorite  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c2c35",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Show the extensive stop word list in spaCy (326 words)\n",
    "- Demonstrate filtering out common, low-information words\n",
    "- Combine multiple preprocessing steps for maximum effect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b0c30-f6b8-4535-88fd-cf0eeb0cf714",
   "metadata": {},
   "source": [
    "## **Section 2: Creating Reusable Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16766b5",
   "metadata": {},
   "source": [
    "Creating modular, reusable functions is essential for maintainable code and consistent preprocessing across different datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e48ecf89-921e-4512-bc3a-68e9c97eaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatization and stop word removal\n",
    "\n",
    "def token_lemmastopw(text):\n",
    "    doc = nlp(text)\n",
    "    output = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(output)\n",
    "\n",
    "# Apply to entire dataset\n",
    "spacy_df[\"clean_sentence\"] = spacy_df[\"clean_sentence\"].apply(token_lemmastopw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea85f8f",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Encapsulate preprocessing logic in a reusable function\n",
    "- Enable consistent processing across multiple texts\n",
    "- Demonstrate functional programming approach to text processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864e731-0adb-4035-8f45-bcd0bdd5a2c9",
   "metadata": {},
   "source": [
    "## **Section 3: Complete NLP Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f2fa1",
   "metadata": {},
   "source": [
    "An **NLP pipeline** combines multiple preprocessing steps into a single, streamlined workflow. This approach ensures consistency and makes it easy to apply the same transformations to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "325777f6-8391-4688-a69d-ba4471e208ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lowercase and replace unwanted patterns\n",
    "def lower_replace(series):\n",
    "    output = series.str.lower()\n",
    "    combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "    output = output.str.replace(combined, ' ', regex=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Complete NLP pipeline\n",
    "def nlp_pipeline(series):\n",
    "    output = lower_replace(series)\n",
    "    output = output.apply(token_lemmastopw)  # make sure function name matches!\n",
    "    return output\n",
    "\n",
    "\n",
    "# Apply complete pipeline\n",
    "cleaned_text = nlp_pipeline(data_df[\"sentence\"])\n",
    "\n",
    "# Save processed data for future use\n",
    "pd.to_pickle(cleaned_text, \"preprocessed_text.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a9677",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Combine all preprocessing steps into a single function\n",
    "- Create a reproducible workflow\n",
    "- Save processed data in pickle format for efficient loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c22a1-92d7-4f65-8dce-d1f7ee6953b4",
   "metadata": {},
   "source": [
    "## **Section 4: Word Representation (Vectorization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd20ff",
   "metadata": {},
   "source": [
    "**Vectorization** converts preprocessed text into numerical representations that machine learning algorithms can process. Text must be transformed into vectors (arrays of numbers) because algorithms cannot directly work with text strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc450b",
   "metadata": {},
   "source": [
    "### **4.1 Count Vectorization (Bag of Words)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46701adb",
   "metadata": {},
   "source": [
    "**Count Vectorization** creates a matrix where each row represents a document and each column represents a unique word in the corpus. Cell values indicate how many times each word appears in each document. This approach ignores word order but captures word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d401fedc-5b68-4dae-a0de-b442a533fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allrecipe</th>\n",
       "      <th>arnold</th>\n",
       "      <th>buy</th>\n",
       "      <th>carry</th>\n",
       "      <th>dozen</th>\n",
       "      <th>eureka</th>\n",
       "      <th>favorite</th>\n",
       "      <th>gallon</th>\n",
       "      <th>give</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>life</th>\n",
       "      <th>market</th>\n",
       "      <th>maven</th>\n",
       "      <th>meyer</th>\n",
       "      <th>palmer</th>\n",
       "      <th>run</th>\n",
       "      <th>sale</th>\n",
       "      <th>tea</th>\n",
       "      <th>today</th>\n",
       "      <th>wikipedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   allrecipe  arnold  buy  carry  dozen  eureka  favorite  gallon  give  \\\n",
       "0          0       0    0      0      0       0         0       0     1   \n",
       "1          0       0    1      0      0       0         0       0     0   \n",
       "2          1       0    0      0      1       0         0       1     0   \n",
       "3          0       0    0      0      0       0         0       0     0   \n",
       "4          0       0    0      0      0       0         0       0     0   \n",
       "5          0       0    0      1      0       1         0       0     0   \n",
       "6          0       1    0      0      0       0         0       0     0   \n",
       "7          0       0    0      0      0       0         1       0     0   \n",
       "\n",
       "   great  ...  life  market  maven  meyer  palmer  run  sale  tea  today  \\\n",
       "0      0  ...     1       0      0      0       0    0     0    0      0   \n",
       "1      0  ...     0       1      1      0       0    0     0    0      0   \n",
       "2      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "3      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "4      1  ...     0       1      0      0       0    1     1    0      1   \n",
       "5      0  ...     0       1      1      1       0    0     0    0      0   \n",
       "6      0  ...     0       0      0      0       1    0     0    1      0   \n",
       "7      0  ...     0       0      0      0       0    0     0    1      0   \n",
       "\n",
       "   wikipedia  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          1  \n",
       "7          0  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load preprocessed data\n",
    "%pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "series = pd.read_pickle('preprocessed_text.pkl')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Create Count Vectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "bow = cv.fit_transform(series)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "\n",
    "pd.DataFrame(bow.toarray(), columns=cv.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417b80a",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Transform text into numerical matrix representation\n",
    "- Each column represents a unique word (feature)\n",
    "- Each cell contains the count of that word in that document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171682f",
   "metadata": {},
   "source": [
    "#### **Advanced Count Vectorization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c7d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count Vectorizer with filtering\n",
    "\n",
    "cv1 = CountVectorizer(\n",
    "\n",
    "stop_words='english', # Remove English stop words\n",
    "\n",
    "ngram_range=(1,1), # Use only single words (unigrams)\n",
    "\n",
    "min_df=2  # Include words that appear in at least 2 documents\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bow1 = cv1.fit_transform(series)\n",
    "\n",
    "bow1_df = pd.DataFrame(bow1.toarray(), columns=cv1.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Calculate term frequencies\n",
    "\n",
    "term_freq = bow1_df.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e1e19",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Apply additional filtering to reduce noise\n",
    "- Focus on words that appear multiple times across documents\n",
    "- Calculate overall term frequencies for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b0c77",
   "metadata": {},
   "source": [
    "## **Section 5: TF-IDF (Term Frequency-Inverse Document Frequency)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b507d",
   "metadata": {},
   "source": [
    "**TF-IDF** addresses a key limitation of simple count vectorization by considering both term frequency (how often a word appears in a document) and inverse document frequency (how rare the word is across the entire corpus).\n",
    "\n",
    "**Formula:** \n",
    "**TF-IDF = TF √ó IDF**\n",
    "\n",
    "**Components:**\n",
    "**TF (Term Frequency):**\n",
    "```\n",
    "TF = Number of times word appears in document / Total words in document\n",
    "```\n",
    "\n",
    "**IDF (Inverse Document Frequency):**\n",
    "```\n",
    "IDF = log(Total documents / Documents containing the word)\n",
    "```\n",
    "\n",
    "**Key Insight:** TF-IDF gives higher weights to words that are frequent in a specific document but rare across the corpus, making them more distinctive and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f141bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Basic TF-IDF vectorization\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "\n",
    "tvidf = tv.fit_transform(series)\n",
    "\n",
    "tvidf_df = pd.DataFrame(tvidf.toarray(), columns=tv.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF with filtering\n",
    "\n",
    "tv1 = TfidfVectorizer(min_df=2) # Words must appear in at least 2 documents\n",
    "\n",
    "tvidf1 = tv1.fit_transform(series)\n",
    "\n",
    "tvidf1_df = pd.DataFrame(tvidf1.toarray(), columns=tv1.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aef36c",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Calculate TF-IDF scores for better feature weighting\n",
    "- Values closer to 1 indicate highly distinctive words\n",
    "- Values closer to 0 indicate either common words or absent words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585527a4",
   "metadata": {},
   "source": [
    "### **N-gram Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca07ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemon                 1.583310\n",
       "lemon lemon           0.857624\n",
       "market                0.767950\n",
       "lemonade              0.743321\n",
       "ice tea               0.625522\n",
       "ice                   0.625522\n",
       "tea                   0.625522\n",
       "maven market          0.621858\n",
       "maven                 0.621858\n",
       "half                  0.505881\n",
       "tea favorite          0.493436\n",
       "favorite              0.493436\n",
       "buy lemon             0.439482\n",
       "buy                   0.439482\n",
       "lemon maven           0.439482\n",
       "life give             0.416207\n",
       "life                  0.416207\n",
       "give                  0.416207\n",
       "give lemon            0.416207\n",
       "lemon lemonade        0.416207\n",
       "lemonade allrecipe    0.358685\n",
       "lemon gallon          0.358685\n",
       "allrecipe             0.358685\n",
       "dozen                 0.358685\n",
       "gallon                0.358685\n",
       "dozen lemon           0.358685\n",
       "gallon lemonade       0.358685\n",
       "run                   0.319884\n",
       "sale today            0.319884\n",
       "market lemon          0.319884\n",
       "sale                  0.319884\n",
       "run market            0.319884\n",
       "great                 0.319884\n",
       "great sale            0.319884\n",
       "today                 0.319884\n",
       "lemon great           0.319884\n",
       "eureka lemon          0.302522\n",
       "lemon meyer           0.302522\n",
       "market carry          0.302522\n",
       "carry eureka          0.302522\n",
       "meyer                 0.302522\n",
       "meyer lemon           0.302522\n",
       "carry                 0.302522\n",
       "eureka                0.302522\n",
       "tea wikipedia         0.252941\n",
       "arnold palmer         0.252941\n",
       "half lemonade         0.252941\n",
       "palmer half           0.252941\n",
       "palmer                0.252941\n",
       "half ice              0.252941\n",
       "lemonade half         0.252941\n",
       "arnold                0.252941\n",
       "wikipedia             0.252941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Bigram TF-IDF (pairs of consecutive words)\n",
    "\n",
    "tv2 = TfidfVectorizer(ngram_range=(1,2)) # Include both unigrams and bigrams\n",
    "\n",
    "tvidf2 = tv2.fit_transform(series)\n",
    "\n",
    "tvidf2_df = pd.DataFrame(tvidf2.toarray(), columns=tv2.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "\n",
    "tvidf2_df.sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb36cc",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Capture phrase-level information with bigrams\n",
    "- Examples: \"arnold palmer\", \"buy lemon\", \"ice tea\"\n",
    "- Preserve some context that unigrams lose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Practical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
